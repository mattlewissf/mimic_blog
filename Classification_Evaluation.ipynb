{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       "html {\n",
       "  font-size: 62.5% !important; }\n",
       "body {\n",
       "  font-size: 1.5em !important; /* currently ems cause chrome bug misinterpreting rems on body element */\n",
       "  line-height: 1.6 !important;\n",
       "  font-weight: 400 !important;\n",
       "  font-family: \"Raleway\", \"HelveticaNeue\", \"Helvetica Neue\", Helvetica, Arial, sans-serif !important;\n",
       "  color: #222 !important; }\n",
       "\n",
       "div{ border-radius: 0px !important;  }\n",
       "div.CodeMirror-sizer{ background: rgb(244, 244, 248) !important; }\n",
       "div.input_area{ background: rgb(244, 244, 248) !important; }\n",
       "\n",
       "h1, h2, h3, h4, h5, h6 {\n",
       "  color: #333 !important;\n",
       "  margin-top: 0 !important;\n",
       "  margin-bottom: 2rem !important;\n",
       "  font-weight: 300 !important; }\n",
       "h1 { font-size: 4.0rem !important; line-height: 1.2 !important;  letter-spacing: -.1rem !important;}\n",
       "h2 { font-size: 3.6rem !important; line-height: 1.25 !important; letter-spacing: -.1rem !important; }\n",
       "h3 { font-size: 3.0rem !important; line-height: 1.3 !important;  letter-spacing: -.1rem !important; }\n",
       "h4 { font-size: 2.4rem !important; line-height: 1.35 !important; letter-spacing: -.08rem !important; }\n",
       "h5 { font-size: 1.8rem !important; line-height: 1.5 !important;  letter-spacing: -.05rem !important; }\n",
       "h6 { font-size: 1.5rem !important; line-height: 1.6 !important;  letter-spacing: 0 !important; }\n",
       "\n",
       "p {\n",
       "  margin-top: 0 !important; }\n",
       "  \n",
       "a {\n",
       "  color: #1EAEDB !important; }\n",
       "a:hover {\n",
       "  color: #0FA0CE !important; }\n",
       "\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "import urllib2\n",
    "HTML(urllib2.urlopen('https://gist.githubusercontent.com/mattlewissf/83989910849fdb4a04a72d431e84053f/raw/cefa015a9065665faccd0219774c7087be7d21a8/skeleton.css').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MIMIC Deep Dive - Understanding How to Evaluate our Classifier\n",
    "**[Intro](#intro)**   \n",
    "**[30 Day Readmission](#30_day_readmission)**  \n",
    "**[The MIMIC Dataset](#mimic_dataset)**  \n",
    "**[Setting up the database](#setting_up_db)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we've reached a point where we can start to understand how well our model does at the task of predicting 30 day readmissions. But before we start importing classifiers and such, it is important that we understand how to evaluate what our model is giving us back, and how to answer a few key questions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a model that can predict the liklihood of patient readmission based on a discharge event is a problem of **classification**. Classification can be understood as two distict tasks: creating clusters within an existing set of data, or 'unsupervised learning'; or classifiying new data into an already understood class, or 'supervised learning'.  Our goal is the latter, as we want to be able to predict whether a given patient (with a given index admission) should be considered as 'a likley readmitter' or not. \n",
    "\n",
    "<br></br> \n",
    "\n",
    "Our model will push each patient into one of two groups (readmit / no readmit). Ignoring for now how we can define 'likely' in our readmission (we'll get back to it), we can start with the a pretty basic questions: how do we know if our model has done a good job? \n",
    "\n",
    "<br></br> \n",
    "\n",
    "\n",
    "#### Classification and how we measure success\n",
    "\n",
    "Since our output is a binary measure of whether or not we think that patient will be readmitted within 30 days, and we are comparing our test data to outcomes where we know the result, we might be tempted intitally to look for something like 'accuracy' that our model produces. In this case, **accuracy** might be defined as: \n",
    "\n",
    "$$ accuracy =  \\frac{\\text{(true positives + true negatives)}}{\\text{all cases}}$$\n",
    "\n",
    "<br></br>\n",
    "\n",
    "However, accuracy isn't going to be super useful for us here. While 95% sounds pretty great, it doesn't tell us all that much. For example, ,a 95% accuracy measure might mean that we have a data set where one state happens only 5% of the time - and all that we've done to earn that 95% accuracy is to never predict the less likely outcome. To quote a [loftier source](http://www.umich.edu/~ners580/ners-bioe_481/lectures/pdfs/1978-10-semNucMed_Metz-basicROC.pdf): \n",
    "> \"Though accuracy provides a single simple number for diagnostic performance, it is often too simple and must be interpreted with\n",
    "considerable caution. The limitations of this index force us to introduce some complexity into our evaluation scheme: We must sort out the effect of disease prevalence, and we must score separately the various kinds of right and wrong diagnostic decisions.\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More useful for our purposes are **sensitivity** (when it is yes, how often does it predict yes?) and **specificity** (when if is no, how often does it predict no?). Additionally, we want to know how often we incorrectly characterized an example. One way to display this information is called a **confusion matrix** <sub>[1](#bib)</sub>: \n",
    "\n",
    "![Fawcett, T. (2006). An introduction to ROC analysis. Pattern Recognition Letters, 27, 861–874.](http://i.imgur.com/cEQmS0f.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thresholds and weights\n",
    "\n",
    "In a perfect world, there would a clear distinction for our model to classify the two groups with a clear cut. But that's not going to be the case with our model. Our model gives us a probability for each person record - but where is the threshold at which we should consider a person to be a 'likely readmit'? \n",
    "\n",
    "The answer is that it depends. If we are very concerned with minimizing false positives, for example, we might have a relatively high threshold for inclusion into the 'readmit' group - however, this will lead to an increase in false negatives. If we were more concerned with grabbing every lilely positive (perhaps for an online advertising campaign), our threshold might be considerably lower. \n",
    "\n",
    "<br></br>\n",
    "\n",
    "The most common way of representing performance over the spectrum of the trade-off between sensitivity and specificity is to plot an **ROC curve**, which has TPR and FPR as it's two axes: \n",
    "\n",
    "![](http://i.imgur.com/7dXmYt0.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph above shows an ROC curve with totally random performance - anything below (to the right and down) of the dotted line is worse than guessing randomly. We'll be creating ROC curves for our model. \n",
    "\n",
    "<br></br>\n",
    "\n",
    "Here's a ROC curve based on just a bit of our data, using a **something** classifier\n",
    "\n",
    "{add image here when you get back}\n",
    "\n",
    "<br></br>\n",
    "\n",
    "{}\n",
    "\n",
    "<br></br>\n",
    "\n",
    "Since the ROC curve is a two-dimensial description of our classifier's performance, we'll want to calculate the Area Under the Curve (AUC) for the ROC curves that we produce. The AUC will be our metric for the general effectiveness of our predictor, with an ideal value of 1.0. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifiers\n",
    "\n",
    "Since we're here to learn, we'll be trying out a lot of different classifiers on for our model, and seeing what gives us the best predictions. All of these are packaged with sk-learn, so beside computation time, there's not a lot of extra effort required to try these on. We'll look into these more when we get our results\n",
    "\n",
    "- RandomForestClassifier,\n",
    "- AdaBoostClassifier\n",
    "- GradientBoostingClassifier\n",
    "- DecisionTreeClassifier\n",
    "- LogisticRegression\n",
    "- LogisticRegressionCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test, train, split\n",
    "\n",
    "We've extracted our features into a pandas dataframe, and now we can easily set up our X and y for this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# separating X and y \n",
    "sk_features = df.columns[1:-1]\n",
    "X = df[sk_features]\n",
    "y = df[\"readmit_30\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to split the data into test and training sets in order to setup our model. We could do this in a simple way (the \"holdout method\") by  just taking some part of both X and y and making those the test and training sets (we'd probably use [sklearn.model_selection.train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). However, we're going to go a bit beyond this, and use **k-fold cross validation** on our data. The k-fold method is similar to the holdout method, but we divide the data set into k subsets, and repeat that holdout method k times. Each time this is done we switch which one of the k-sets is being used as the test set, and all of the other k-sets are combined into a training set. \n",
    "\n",
    "<br></br>\n",
    "<img src='http://www.mdpi.com/sensors/sensors-12-12489/article_deploy/html/images/sensors-12-12489f7-1024.png' width=\"900\" height=\"400\" align='float' margin-right=50px />\n",
    "\n",
    "<br></br>\n",
    "\n",
    "We'll use sk-learn's very convienent [KFold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold) method to help us with this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits = 10) # 10 is a generally recommended value\n",
    "kf.get_n_splits(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll then used each of these separate folds to fit our classifier to the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  **Next  |   [Fitting and Plotting]()**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sources\n",
    "\n",
    "- Fawcett, T. (2006). An introduction to ROC analysis. Pattern Recognition Letters, 27, 861–874"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
